{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "INFO5731_Assignment_Two.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RomalPatel28/Romal_INFO5731_Fall2021/blob/main/INFO5731_Assignment_Two.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "USSdXHuqnwv9"
      },
      "source": [
        "# **INFO5731 Assignment Two**\n",
        "\n",
        "In this assignment, you will try to gather text data from open data source via web scraping or API. After that you need to clean the text data and syntactic analysis of the data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YWxodXh5n4xF"
      },
      "source": [
        "# **Question 1**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TenBkDJ5n95k"
      },
      "source": [
        "(20 points). Write a python program to collect text data from **either of the following sources** and save the data into a **csv file**:\n",
        "\n",
        "(1) Collect all the customer reviews of the product [Apple iPhone 11](https://www.amazon.com/Apple-iPhone-11-64GB-Unlocked/dp/B07ZPKF8RG/ref=sr_1_13?dchild=1&keywords=iphone+12&qid=1631721363&sr=8-13) on amazon.\n",
        "\n",
        "(2) Collect all User Reviews of the film [Shang-Chi and the Legend of the Ten Rings](https://www.imdb.com/title/tt9376612/reviews?ref_=tt_sa_3) from IMDB.\n",
        "\n",
        "(3) Collect all the abstracts research papers by using the query [smart health](https://citeseerx.ist.psu.edu/search?q=natural+language+processing&submit.x=0&submit.y=0&sort=rlv&t=doc) from CiteSeerX.\n",
        "\n",
        "(4) Collect the top 10,000 tweets by using hashtag [\"#blacklivesmatter\"](https://twitter.com/hashtag/blacklivesmatter) from Twitter with the following requirements: Location (Texas), Time frame (2021-01-01 to 2021-09-01). \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "id": "PuFPKhC0m1fd",
        "outputId": "176b2865-3c03-43f3-ee0a-ba9de3fc1a67"
      },
      "source": [
        "# Write your code here\n",
        "# importing neccessary packages\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "from bs4 import BeautifulSoup\n",
        "import urllib.request\n",
        "\n",
        "url = \"https://citeseerx.ist.psu.edu/search?q=natural+language+processing&submit.x=0&submit.y=0&sort=rlv&t=doc\"\n",
        "page = urllib.request.urlopen(url)\n",
        "\n",
        "#Parse the html in the 'page' variable and store it in Beautiful Soup\n",
        "soup = BeautifulSoup(page)\n",
        "\n",
        "title = []\n",
        "for new_title in soup.find_all('a',{'class':'remove doc_details'}):\n",
        "  new_title = title.append(new_title.text.strip())\n",
        "\n",
        "year = []\n",
        "for new_year in soup.find_all(\"span\",{'class':'pubyear'}):\n",
        "  new_year = year.append(new_year.text.strip())\n",
        "\n",
        "authors = []\n",
        "for author_name in soup.find_all('span',{'class':'authors'}):\n",
        "  author_name = authors.append(author_name.text.strip())\n",
        "\n",
        "abstract = []\n",
        "for sub in soup.find_all(\"div\",{'class': 'snippet'}):\n",
        "  sub = abstract.append(sub.text.strip())\n",
        "\n",
        "new_dict = {'Title':title,'Year':year,'Authors':authors,'Abstract':abstract}\n",
        "\n",
        "NLParticles = pd.DataFrame.from_dict(new_dict, orient='index')\n",
        "NLParticles.transpose()\n",
        "\n",
        "new_file = NLParticles.to_csv(\"NLP_articles.csv\")\n",
        "NLParticles\n",
        "\n",
        "\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Title</th>\n",
              "      <td>Foundations of statistical natural language pr...</td>\n",
              "      <td>A Maximum Entropy approach to Natural Language...</td>\n",
              "      <td>Natural Language Processing</td>\n",
              "      <td>Linguistics and Natural Language Processing</td>\n",
              "      <td>Natural Language Processing</td>\n",
              "      <td>Natural language processing (almost) from scratch</td>\n",
              "      <td>Natural language processing</td>\n",
              "      <td>Natural Language Processing/Robotics</td>\n",
              "      <td>Tutorial on Natural Language Processing</td>\n",
              "      <td>Ambiguities in Natural Language Processing</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Year</th>\n",
              "      <td>, 2000</td>\n",
              "      <td>, 1996</td>\n",
              "      <td>, 2007</td>\n",
              "      <td>, 1987</td>\n",
              "      <td>, 2001</td>\n",
              "      <td>, 2011</td>\n",
              "      <td>, 1996</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Authors</th>\n",
              "      <td>by \\n                  Christopher D. Manning,...</td>\n",
              "      <td>by \\n                  Adam L. Berger, Stephen...</td>\n",
              "      <td>by \\n                  Cohn Trevor A, Trevor A...</td>\n",
              "      <td>by \\n                  Victor Raskin</td>\n",
              "      <td>by \\n                  Enrico Franconi</td>\n",
              "      <td>by \\n                  Ronan Collobert, Jason ...</td>\n",
              "      <td>by \\n                  Mark Steedman, Linc Lab...</td>\n",
              "      <td>by \\n                  Timothy Brick</td>\n",
              "      <td>by \\n                  unknown authors</td>\n",
              "      <td>by \\n                  Anjali M , Babu Anto</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Abstract</th>\n",
              "      <td>\"...   ...\"</td>\n",
              "      <td>\"... The concept of maximum entropy can be tra...</td>\n",
              "      <td>\"... Scaling conditional random fields for nat...</td>\n",
              "      <td>\"... The paper addresses the issue of cooperat...</td>\n",
              "      <td>\"... In most natural language processing appli...</td>\n",
              "      <td>\"... We propose a unified neural network archi...</td>\n",
              "      <td>\"... Natural Language Processing The subject o...</td>\n",
              "      <td>\"... Robots that interact with humans face-to-...</td>\n",
              "      <td>\"... Natural languages are languages spoken by...</td>\n",
              "      <td>\"...  ABSTRACT: Ambiguity can be referred as t...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                          0  ...                                                  9\n",
              "Title     Foundations of statistical natural language pr...  ...         Ambiguities in Natural Language Processing\n",
              "Year                                                 , 2000  ...                                               None\n",
              "Authors   by \\n                  Christopher D. Manning,...  ...        by \\n                  Anjali M , Babu Anto\n",
              "Abstract                                        \"...   ...\"  ...  \"...  ABSTRACT: Ambiguity can be referred as t...\n",
              "\n",
              "[4 rows x 10 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AfpMRCrRwN6Z"
      },
      "source": [
        "# **Question 2**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1dCQEbDawWCw"
      },
      "source": [
        "(30 points) Here is a [legal case](https://github.com/unt-iialab/info5731-fall2021/blob/main/assignments/01-05-1%20%20Adams%20v%20Tanner.txt) we collected from westlaw, please follow the steps we mentioned in lesson 5 to clean the data:\n",
        "\n",
        "\n",
        "\n",
        "(1) Basic feature extraction using text data\n",
        "\n",
        "*   Number of sentences\n",
        "*   Number of words\n",
        "*   Number of characters\n",
        "*   Average word length\n",
        "*   Number of stopwords\n",
        "*   Number of special characters\n",
        "*   Number of numerics\n",
        "*   Number of uppercase words\n",
        "\n",
        "(2) Basic Text Pre-processing of text data\n",
        "\n",
        "*   Lower casing\n",
        "*   Punctuation removal\n",
        "*   Stopwords removal\n",
        "*   Frequent words removal\n",
        "*   Rare words removal\n",
        "*   Spelling correction\n",
        "*   Tokenization\n",
        "*   Stemming\n",
        "*   Lemmatization\n",
        "\n",
        "(3) Save all the **clean sentences** to a **csv file** (one column, each raw is a sentence) after finishing all the steps above.\n",
        "\n",
        "\n",
        "(4) Advance Text Processing\n",
        "\n",
        "*   Calculate the term frequency of all the terms.\n",
        "*   Print out top 10 1-gram, top 10 2-grams, and top 10 3-grams terms as features.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 417
        },
        "id": "vATjQNTY8buA",
        "outputId": "39e2cd8f-c6f4-4818-cb66-5addf12c5817"
      },
      "source": [
        "# Write your code here\n",
        "# 2.\n",
        "import pandas as pd\n",
        "dataframe = pd.read_csv(\"legal_case.txt\", sep=\"\\n\")\n",
        "dataframe.rename(columns = {'5 Ala. 740':'legal_cases'}, inplace = True)\n",
        "\n",
        "# Number of words\n",
        "dataframe['word_count'] = dataframe['legal_cases'].apply(lambda x: len(str(x).split(\" \")))\n",
        "\n",
        "# character count\n",
        "dataframe['char_count'] = dataframe['legal_cases'].str.len()\n",
        "\n",
        "# Averge word length\n",
        "def average(sentence):\n",
        "  words = sentence.split()\n",
        "  try:\n",
        "    return (sum(len(i) for i in words)/len(words))\n",
        "  except ZeroDivisionError:\n",
        "    words = 0 \n",
        "\n",
        "dataframe['average word'] = dataframe['legal_cases'].apply(lambda x: average(x))\n",
        "\n",
        "# Uppercase\n",
        "dataframe['upper'] = dataframe['legal_cases'].apply(lambda x: len([x for x in x.split() if x.isupper()]))\n",
        "dataframe\n"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>legal_cases</th>\n",
              "      <th>word_count</th>\n",
              "      <th>char_count</th>\n",
              "      <th>average word</th>\n",
              "      <th>upper</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Supreme Court of Alabama.</td>\n",
              "      <td>4</td>\n",
              "      <td>25</td>\n",
              "      <td>5.500000</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ADAMS</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>v.</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>TANNER AND HORTON.</td>\n",
              "      <td>3</td>\n",
              "      <td>18</td>\n",
              "      <td>5.333333</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>June Term, 1843.</td>\n",
              "      <td>3</td>\n",
              "      <td>16</td>\n",
              "      <td>4.666667</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>142</th>\n",
              "      <td>There are no Filings for this citation.</td>\n",
              "      <td>7</td>\n",
              "      <td>39</td>\n",
              "      <td>4.714286</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>143</th>\n",
              "      <td>Negative Treatment</td>\n",
              "      <td>2</td>\n",
              "      <td>18</td>\n",
              "      <td>8.500000</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>144</th>\n",
              "      <td>There are no Negative Treatment results for th...</td>\n",
              "      <td>9</td>\n",
              "      <td>58</td>\n",
              "      <td>5.555556</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>145</th>\n",
              "      <td>History</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>7.000000</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>146</th>\n",
              "      <td>There are no History results for this citation.</td>\n",
              "      <td>8</td>\n",
              "      <td>47</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>147 rows × 5 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                           legal_cases  ...  upper\n",
              "0                            Supreme Court of Alabama.  ...      0\n",
              "1                                                ADAMS  ...      1\n",
              "2                                                   v.  ...      0\n",
              "3                                   TANNER AND HORTON.  ...      3\n",
              "4                                     June Term, 1843.  ...      0\n",
              "..                                                 ...  ...    ...\n",
              "142            There are no Filings for this citation.  ...      0\n",
              "143                                 Negative Treatment  ...      0\n",
              "144  There are no Negative Treatment results for th...  ...      0\n",
              "145                                            History  ...      0\n",
              "146    There are no History results for this citation.  ...      0\n",
              "\n",
              "[147 rows x 5 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 417
        },
        "id": "kWoPIFQ6CEZm",
        "outputId": "912c00da-91d4-4313-80d1-ea7c52632918"
      },
      "source": [
        "\n"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>legal_cases</th>\n",
              "      <th>word_count</th>\n",
              "      <th>char_count</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Supreme Court of Alabama.</td>\n",
              "      <td>4</td>\n",
              "      <td>25</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ADAMS</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>v.</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>TANNER AND HORTON.</td>\n",
              "      <td>3</td>\n",
              "      <td>18</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>June Term, 1843.</td>\n",
              "      <td>3</td>\n",
              "      <td>16</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>142</th>\n",
              "      <td>There are no Filings for this citation.</td>\n",
              "      <td>7</td>\n",
              "      <td>39</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>143</th>\n",
              "      <td>Negative Treatment</td>\n",
              "      <td>2</td>\n",
              "      <td>18</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>144</th>\n",
              "      <td>There are no Negative Treatment results for th...</td>\n",
              "      <td>9</td>\n",
              "      <td>58</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>145</th>\n",
              "      <td>History</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>146</th>\n",
              "      <td>There are no History results for this citation.</td>\n",
              "      <td>8</td>\n",
              "      <td>47</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>147 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                           legal_cases  word_count  char_count\n",
              "0                            Supreme Court of Alabama.           4          25\n",
              "1                                                ADAMS           1           5\n",
              "2                                                   v.           1           2\n",
              "3                                   TANNER AND HORTON.           3          18\n",
              "4                                     June Term, 1843.           3          16\n",
              "..                                                 ...         ...         ...\n",
              "142            There are no Filings for this citation.           7          39\n",
              "143                                 Negative Treatment           2          18\n",
              "144  There are no Negative Treatment results for th...           9          58\n",
              "145                                            History           1           7\n",
              "146    There are no History results for this citation.           8          47\n",
              "\n",
              "[147 rows x 3 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W5vCcrMUB_CM"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EVBGPMXK_q-R",
        "outputId": "e611d2b5-248e-49de-fecf-a2858a540f5c"
      },
      "source": [
        "# Lower case\n",
        "dataframe['legal_cases'] = dataframe['legal_cases'].apply(lambda x: \" \".join(x.lower() for x in x.split()))\n",
        "\n",
        "# Removing Punctuations\n",
        "import string\n",
        "dataframe['legal_cases'] = dataframe['legal_cases'].str.replace('[{}]'.format(string.punctuation), '')\n",
        "\n",
        "# Removing Stopwords\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "stop=stopwords.words('english')\n",
        "dataframe['legal_cases'] = dataframe['legal_cases'].apply(lambda x:\" \".join(x for x in x.split() if x not in stop))\n",
        "\n",
        "# frequent word removal\n",
        "freq=pd.Series(' '.join(dataframe['legal_cases']).split()).value_counts()[:10]\n",
        "freq=list(freq.index)\n",
        "dataframe['legal_cases'] = dataframe['legal_cases'].apply(lambda x:\" \".join(x for x in x.split() if x not in freq))\n",
        "\n",
        "# Rar word removal\n",
        "freq=pd.Series(' '.join(dataframe['legal_cases']).split()).value_counts()[-10:]\n",
        "freq=list(freq.index)\n",
        "dataframe['Legal_cases'] = dataframe['legal_cases'].apply(lambda x:\" \".join(x for x in x.split() if x not in freq))\n",
        "\n",
        "# Spelling Correction\n",
        "from textblob import TextBlob\n",
        "dataframe['legal_cases'] = dataframe['legal_cases'].apply(lambda x: str(TextBlob(x).correct()))\n",
        "\n",
        "# Stemming\n",
        "from nltk.stem import PorterStemmer\n",
        "st=PorterStemmer()\n",
        "dataframe['legal_cases'] = dataframe['legal_cases'].apply(lambda x: \" \".join([st.stem(word) for word in x.split()]))\n",
        "\n",
        "# Lemmatization\n",
        "from textblob import Word\n",
        "nltk.download('wordnet')\n",
        "dataframe['legal_cases'] = dataframe['legal_cases'].apply(lambda x: \" \".join([Word(word).lemmatize() for word in x.split()]))\n",
        "\n",
        "# Tokenization\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "dataframe['legal_cases'] = dataframe['legal_cases'].apply(lambda x: TextBlob(x).words)\n",
        "\n",
        "# (3) Saving to a csv file\n",
        "dataframe.to_csv('processed_file.csv')\n",
        "print(dataframe)\n"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "                          legal_cases  ...                          Legal_cases\n",
            "0                   [suprem, alabama]  ...                      supreme alabama\n",
            "1                              [adam]  ...                                adams\n",
            "2                                  []  ...                                     \n",
            "3                    [manner, norton]  ...                        tanner horton\n",
            "4                        [june, 1843]  ...                            june 1843\n",
            "..                                ...  ...                                  ...\n",
            "142                    [file, situat]  ...                     filings citation\n",
            "143                  [neg, treatment]  ...                   negative treatment\n",
            "144  [neg, treatment, result, situat]  ...  negative treatment results citation\n",
            "145                         [histori]  ...                              history\n",
            "146         [histori, result, situat]  ...             history results citation\n",
            "\n",
            "[147 rows x 4 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E5mmYIfN8eYV"
      },
      "source": [
        "# **Question 3**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hsi2y4z88ngX"
      },
      "source": [
        "(30 points). Write a python program to conduct **syntax and structure analysis** of the clean text you just saved above. The syntax and structure analysis includes: \n",
        "\n",
        "(1) Parts of Speech (POS) Tagging: Tag Parts of Speech of each word in the text, and calculate the total number of N(oun), V(erb), Adj(ective), Adv(erb), respectively.\n",
        "\n",
        "(2) Constituency Parsing and Dependency Parsing: print out the constituency parsing trees and dependency parsing trees of all the sentences. Using one sentence as an example to explain your understanding about the constituency parsing tree and dependency parsing tree.\n",
        "\n",
        "(3) Named Entity Recognition: Extract all the entities such as person names, organizations, locations, product names, and date from the clean texts, calculate the count of each entity."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QQKnPjPDHJHr"
      },
      "source": [
        "# Write your code here\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xWOtvT2rHNWy"
      },
      "source": [
        "**Write your explanations of the constituency parsing tree and dependency parsing tree here (Question 3-2):** "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uAXy0Wsb5Lvb"
      },
      "source": [
        "# You explaination here\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KVkPIwqq5Lvc"
      },
      "source": [
        "# **Question 4**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SL-4qR6c5Lvc"
      },
      "source": [
        "(20 points) Python Regular Expression.\n",
        "\n",
        "(1) Write a Python program to remove leading zeros from an IP address.\n",
        "\n",
        "ip = \"260.08.094.109\"\n",
        "\n",
        "\n",
        "(2) Write a Python Program to extract all the years from the following sentence.\n",
        "\n",
        "sentence = \"The 2010s were a dramatic decade, filled with ups and downs, more than 1000 stroies have happened. As the decade comes to a close, Insider took a look back at some of the biggest headline-grabbing stories, from 2010 to 2019. The result was 119 news stories that ranged from the heartwarming rescue of a Thai boys' soccer team from a flooded cave to the divisive election of President Donald Trump.\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EtnIbz8Z5Lvd",
        "outputId": "f3c4854d-fd85-489c-88b0-4ef2a0a81c9d"
      },
      "source": [
        "# 1. Using a function to keep track of the zeros\n",
        "ipadd = \"260.08.094.109\" \n",
        "def original_sentence(ipadd):\n",
        "  no_leading0 = \".\".join([str(int(j)) for j in ipadd.split(\".\")]) \n",
        "  return no_leading0;\n",
        "\n",
        "print(\"The new IP address after removing the leading zeroes is: \" + original_sentence(ipadd))\n",
        "\n",
        "# 2. Extract all the years from the following sentence\n",
        "import re\n",
        "year_sentence = \"The 2010s were a dramatic decade, filled with ups and downs, more than 1000 stroies have happened. As the decade comes to a close, Insider took a look back at some of the biggest headline-grabbing stories, from 2010 to 2019. The result was 119 news stories that ranged from the heartwarming rescue of a Thai boys' soccer team from a flooded cave to the divisive election of President Donald Trump.\"\n",
        "no_year = re.findall('2\\d{3}', year_sentence)\n",
        "print(\"The extracted years are: \" + str(no_year))\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The new IP address after removing the leading zeroes is: 260.8.94.109\n",
            "The extracted years are: ['2010', '2010', '2019']\n"
          ]
        }
      ]
    }
  ]
}